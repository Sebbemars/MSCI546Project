{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebbemars/MSCI546Project/blob/main/546_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RBnATGadTG3"
      },
      "source": [
        "# **Install Pytorch Lightning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rrXmZp-dWPJ"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h-N1wGaQ6pI"
      },
      "source": [
        "# **Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og4oXihZQ9cG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "#from pytorch_lightning.profiler import SimpleProfiler\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create RMSLE\n",
        "\n",
        "class RMSLELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        \n",
        "    def forward(self, pred, actual):\n",
        "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
      ],
      "metadata": {
        "id": "eagd1YRAxZRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6aFY5jeOA9S"
      },
      "source": [
        "# **Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeFTN8MmAwB7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVLoM4rQBJng"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'drive/MyDrive/output.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQF5rlmbRI6B"
      },
      "outputs": [],
      "source": [
        "raw_data_train1 = pd.read_csv('drive/MyDrive/train.csv') #10886\n",
        "raw_data_test = pd.read_csv('drive/MyDrive/test.csv') #6493\n",
        "\n",
        "raw_data_train = pd.concat([raw_data_train1,raw_data_test]) #this one is actually the train and test put together but im to lazy to change all names sorry\n",
        "\n",
        "\n",
        "#raw_data_train.describe()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/totaloutput.csv', 'w', encoding = 'utf-8-sig') as f:\n",
        "  raw_data_train.to_csv(f)"
      ],
      "metadata": {
        "id": "KqeXrUnxAcUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/head.csv', 'w', encoding = 'utf-8-sig') as f:\n",
        "  raw_data_train.head().to_csv(f)\n"
      ],
      "metadata": {
        "id": "DNNlU3c4CExq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTuGMpkJOFOi"
      },
      "source": [
        "# **Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqgmHmY8cFNi"
      },
      "outputs": [],
      "source": [
        "# Data has both categorial data and numerical data, but for this tutorial \n",
        "# we will use just the numerical data.\n",
        "numeric_columns = []\n",
        "numeric_columns.extend(list(raw_data_train.dtypes[raw_data_train.dtypes == np.int64].index))\n",
        "numeric_columns.extend(list(raw_data_train.dtypes[raw_data_train.dtypes == np.float64].index))\n",
        "\n",
        "\n",
        "# Remove atemp since its basically temp since we don't need it\n",
        "numeric_columns.remove('atemp')\n",
        "\n",
        "numeric_data_tr = DataFrame(raw_data_train, columns=numeric_columns)\n",
        "\n",
        "nan_columns_tr = np.any(pd.isna(numeric_data_tr), axis = 0)\n",
        "nan_columns_tr = list(nan_columns_tr[nan_columns_tr == True].index)\n",
        "\n",
        "\n",
        "# Fill NaN values with 0\n",
        "for col in nan_columns_tr:\n",
        "    numeric_data_tr[col] = numeric_data_tr[col].fillna(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBLfcXmLdAv6"
      },
      "outputs": [],
      "source": [
        "numeric_x_columns = list(numeric_data_tr.columns)\n",
        "numeric_x_columns.remove('count')\n",
        "numeric_x_columns.remove('windspeed')\n",
        "numeric_x_columns.remove('registered')\n",
        "numeric_x_columns.remove('casual')   #Removing these ended up reducing performance\n",
        "numeric_y_columns = ['count']\n",
        "\n",
        "numeric_tr_x_df = DataFrame(numeric_data_tr, columns=numeric_x_columns)\n",
        "numeric_tr_y_df = DataFrame(numeric_data_tr, columns=numeric_y_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE! clicking this will reduce performance\n",
        "# Noramlize the data!, this did not improve performance weirdly enough\n",
        "\n",
        "numeric_data_tr_norm = (numeric_data_tr - numeric_data_tr.mean()) / numeric_data_tr.std()\n",
        "numeric_x_columns = list(numeric_data_tr_norm.columns)\n",
        "numeric_x_columns.remove('count')\n",
        "#numeric_x_columns.remove('windspeed')\n",
        "#numeric_x_columns.remove('registered')\n",
        "#numeric_x_columns.remove('casual')\n",
        "\n",
        "numeric_tr_x_df = DataFrame(numeric_data_tr_norm, columns=numeric_x_columns)\n",
        "numeric_tr_x_df.head()\n",
        "numeric_y_columns = ['count']\n",
        "numeric_tr_y_df = DataFrame(numeric_data_tr, columns=numeric_y_columns)\n"
      ],
      "metadata": {
        "id": "TF6MHYfEFjkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BzP_D65Ou7V"
      },
      "source": [
        "# **Define DataModule for PL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYwV3Sd7dGcE"
      },
      "outputs": [],
      "source": [
        "class BikeSharingDemandDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file_x, csv_file_y):\n",
        "        self.csv_file_x = torch.tensor(csv_file_x.values, dtype=torch.float)\n",
        "        self.csv_file_y = torch.tensor(csv_file_y.values, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.csv_file_x[idx]\n",
        "        y = self.csv_file_y[idx]\n",
        "        return {'x': x, 'y': y}\n",
        "\n",
        "\n",
        "class BikeSharingDemandDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_size = kwargs.get('batch_size')\n",
        "        self.num_workers = kwargs.get('num_workers', 0)\n",
        "        self.val_ratio = kwargs.get('val_ratio')\n",
        "\n",
        "        error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
        "        assert ((self.val_ratio >= 0) and (self.val_ratio <= 1)), error_msg\n",
        "        # Data: data transformation strategy\n",
        "\n",
        "\n",
        "\n",
        "        num_train = 10886\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.val_ratio * num_train))\n",
        "\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "        test_idx = list(range(10887, 17379))\n",
        "\n",
        "        self.dataset_tr = BikeSharingDemandDataset(numeric_tr_x_df.iloc[train_idx], numeric_tr_y_df.iloc[train_idx])\n",
        "        self.dataset_val = BikeSharingDemandDataset(numeric_tr_x_df.iloc[valid_idx], numeric_tr_y_df.iloc[valid_idx])\n",
        "        self.dataset_te = BikeSharingDemandDataset(numeric_tr_x_df.iloc[test_idx], numeric_tr_y_df.iloc[test_idx])\n",
        "\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.dataset_tr, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.dataset_val, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.dataset_te, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs6OmR3gPKjt"
      },
      "source": [
        "# **Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s76J2n3rxxTg"
      },
      "outputs": [],
      "source": [
        "class Regression(pl.LightningModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.lr = kwargs.get('lr')\n",
        "\n",
        "        self.linear1 = nn.Linear(6, 100)\n",
        "        self.linear2 = nn.Linear(100, 200)\n",
        "        self.linear3 = nn.Linear(200, 200)\n",
        "        self.linear4 = nn.Linear(200, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.loss = RMSLELoss()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = self.relu(self.linear1(x))\n",
        "        y_pred = self.relu(self.linear2(y_pred))\n",
        "        y_pred = self.relu(self.linear3(y_pred))\n",
        "        y_pred = self.relu(self.linear4(y_pred))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch['x'], batch['y']\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss(y_hat, y)\n",
        "        self.log('Training loss', loss.item())\n",
        "        return loss\n",
        "\n",
        "    def on_validation_start(self):\n",
        "        self.losses = []\n",
        "        self.num_samples = 0\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch['x'], batch['y']\n",
        "        y_hat = self(x)\n",
        "        loss = torch.pow(torch.log(y_hat + 1)-torch.log(y + 1),2)\n",
        "        self.num_samples += x.size(0)\n",
        "        self.losses.append(loss.sum().item())\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        overall_loss = np.sqrt(np.sum(self.losses)/self.num_samples)\n",
        "        self.log('Validation loss', overall_loss)\n",
        "\n",
        "        \n",
        "\n",
        "    def on_test_start(self):\n",
        "        self.losses = []\n",
        "        self.num_samples = 0\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch['x'], batch['y']\n",
        "        y_hat = self(x)\n",
        "        loss = torch.pow(torch.log(y_hat + 1)-torch.log(y + 1),2)\n",
        "        self.num_samples += x.size(0)\n",
        "        self.losses.append(loss.sum().item())\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        overall_loss = np.sqrt(np.sum(self.losses)/self.num_samples)\n",
        "        self.log('Test loss', overall_loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqLuXUM11vFu"
      },
      "source": [
        "# **Define Training Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJkyESmT1zDa"
      },
      "outputs": [],
      "source": [
        "dict_args = {\n",
        "    'dataloader': BikeSharingDemandDataModule,\n",
        "    'load': None,\n",
        "    'resume_from_checkpoint': None,\n",
        "    'batch_size': 32,\n",
        "    'epoch': 50,\n",
        "    'num_workers': 0, \n",
        "    'val_freq': 0.5, \n",
        "    'logdir': './logs',\n",
        "    'lr': 0.001, \n",
        "    'display_freq': 64,\n",
        "    'seed': 42, \n",
        "    'clip_grad_norm': 0, \n",
        "    'val_ratio': 0.2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPIibrHb2hGd"
      },
      "outputs": [],
      "source": [
        "# Define seed for reproducibility\n",
        "pl.seed_everything(dict_args['seed'])\n",
        "# Initialize model to train\n",
        "\n",
        "if dict_args['load'] is not None:\n",
        "    model = Regression.load_from_checkpoint(dict_args['load'], **dict_args)\n",
        "else:\n",
        "    model = Regression(**dict_args)\n",
        "\n",
        "# Initialize logging paths\n",
        "now = datetime.datetime.now().strftime('%m%d%H%M%S')\n",
        "print(now)\n",
        "weight_save_dir = os.path.join(dict_args[\"logdir\"], os.path.join('models', 'state_dict', now))\n",
        "\n",
        "\n",
        "os.makedirs(weight_save_dir, exist_ok=True)\n",
        "\n",
        "# Callback: model checkpoint strategy\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=weight_save_dir, save_top_k=5, verbose=True, monitor=\"Validation loss\", mode=\"min\"\n",
        ")\n",
        "\n",
        "# Data: load data module\n",
        "data_module = dict_args['dataloader'](**dict_args)\n",
        "\n",
        "# Trainer: initialize training behaviour\n",
        "\n",
        "logger = TensorBoardLogger(save_dir=dict_args['logdir'], version=now, name='lightning_logs', log_graph=True)\n",
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback],\n",
        "    val_check_interval=dict_args['val_freq'],\n",
        "    deterministic=True,\n",
        "    logger=logger,\n",
        "    max_epochs=dict_args[\"epoch\"],\n",
        "    log_every_n_steps=dict_args[\"display_freq\"],\n",
        "    gradient_clip_val=dict_args['clip_grad_norm'],\n",
        "    #resume_from_checkpoint=dict_args['resume_from_checkpoint']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rjn6B4X2n0T"
      },
      "source": [
        "# **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiPlIPQB2qnb"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdnvPrw42Mv"
      },
      "source": [
        "# **Test the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vf0_7Xk5AbX"
      },
      "outputs": [],
      "source": [
        "trainer.test(model, ckpt_path='best', datamodule=data_module)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}